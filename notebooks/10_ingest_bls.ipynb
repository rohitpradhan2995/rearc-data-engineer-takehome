{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaec6e49-fde4-40d0-83bc-4eaf5b46b011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BLS PR Raw Sync (Mirror Ingestion) + Run Metadata (JSON)\n",
    "\n",
    "This notebook performs a **raw-zone mirror sync** of the BLS *PR time series* files from the public BLS directory into the lakehouse:\n",
    "\n",
    "- **Source:** `https://download.bls.gov/pub/time.series/pr/`\n",
    "- **Target:** `/Volumes/rearc_quest/lakehouse/raw_bls`\n",
    "\n",
    "## What it does\n",
    "1. **Discovers upstream files** by parsing the BLS directory listing and selecting only `pr.*` files.\n",
    "2. **Downloads each file** using an HTTP session configured with **retry/backoff** for transient failures (429 / 5xx).\n",
    "3. **Writes new files** to the target directory.\n",
    "4. **Updates existing files** only when content changes, using **strict full-file SHA-256 hashing** (not timestamp-based).\n",
    "5. **Deletes local files** that no longer exist upstream when `ENABLE_DELETE=True` (mirror behavior).\n",
    "   - Uses `PROTECTED_FILES` as guardrails (never delete these).\n",
    "\n",
    "## Observability / Audit trail\n",
    "Every run writes metadata as JSON:\n",
    "- **Per-run file (append-only):** `.../_meta/runs/<run_id>.json`\n",
    "- **Latest pointer:** `.../_meta/latest.json`\n",
    "\n",
    "The metadata includes:\n",
    "- run timestamps, duration, status\n",
    "- counts (uploaded/updated/skipped/deleted)\n",
    "- any per-file errors encountered\n",
    "\n",
    "## Notes\n",
    "- It is a **raw ingestion** pattern: preserve upstream file boundaries and overwrite only when content differs.\n",
    "- Using a stable content hash makes reruns **idempotent** and avoids unnecessary rewrites.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8f1bb4-516e-4486-b235-67544f6a27be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "import datetime as dt\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DATA_SOURCE = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "TARGET_DIR  = \"/Volumes/rearc_quest/lakehouse/raw_bls\"\n",
    "\n",
    "# Mirror-delete behavior:\n",
    "# - True  : delete local files not present upstream (mirror sync)\n",
    "# - False : non-destructive (keep local files even if upstream removes them)\n",
    "ENABLE_DELETE = True\n",
    "\n",
    "# Guardrails: never delete these even if ENABLE_DELETE=True\n",
    "PROTECTED_FILES = {\"population.json\"}\n",
    "\n",
    "USER_AGENT = \"rearc-quest-contact: rohit.pradhan2995@gmail.com\"\n",
    "\n",
    "# Run metadata paths (JSON-only history)\n",
    "META_DIR     = f\"{TARGET_DIR}/_meta\"\n",
    "RUNS_DIR     = f\"{META_DIR}/runs\"\n",
    "LATEST_PATH  = f\"{META_DIR}/latest.json\"\n",
    "\n",
    "dbutils.fs.mkdirs(TARGET_DIR)\n",
    "dbutils.fs.mkdirs(META_DIR)\n",
    "dbutils.fs.mkdirs(RUNS_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Robust HTTP session (retry/backoff for 429/5xx)\n",
    "# ----------------------------\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1.5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    "    raise_on_status=False\n",
    ")\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT, \"Accept\": \"*/*\"})\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def sha256_bytes(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def read_local_bytes(volume_path: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Full-file read for strict correctness hashing.\n",
    "    Tries direct Volume path, then /dbfs fallback.\n",
    "    \"\"\"\n",
    "    candidates = [volume_path]\n",
    "    if not volume_path.startswith(\"/dbfs\"):\n",
    "        candidates.append(\"/dbfs\" + volume_path)\n",
    "\n",
    "    last_err = None\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def list_local_files(target_dir: str):\n",
    "    try:\n",
    "        return sorted([f.name for f in dbutils.fs.ls(target_dir) if f.isFile()])\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def list_remote_files():\n",
    "    \"\"\"\n",
    "    Discover upstream files by parsing the BLS directory listing.\n",
    "    Uses multiple patterns to handle slightly different listing formats.\n",
    "    \"\"\"\n",
    "    r = session.get(DATA_SOURCE, timeout=60, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    text = r.text\n",
    "\n",
    "    patterns = [\n",
    "        r'href=\"(pr\\.[^\"]+)\"',\n",
    "        r\"href='(pr\\.[^']+)'\",\n",
    "        r'href=(pr\\.[^\\s>]+)',\n",
    "        r'>(pr\\.[^<\\s]+)<',\n",
    "    ]\n",
    "    files = set()\n",
    "    for p in patterns:\n",
    "        files.update(re.findall(p, text))\n",
    "\n",
    "    # Defensive: keep scope tight to `pr.*`\n",
    "    return sorted([f for f in files if f and f.startswith(\"pr.\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230309e7-b3dd-4d34-8510-7a4ba9f39a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-6006fdae-84e1-463f-8a94-8c/.ipykernel/2516/command-5096760691136219-1274913725:4: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  start_ts = dt.datetime.utcnow()\n/home/spark-6006fdae-84e1-463f-8a94-8c/.ipykernel/2516/command-5096760691136219-1274913725:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  run_id = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\") + \"_\" + hashlib.sha256(run_id_seed.encode(\"utf-8\")).hexdigest()[:8]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote files: 12\nUploaded=0, Updated=0, Skipped=12\nDeleted files: 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-6006fdae-84e1-463f-8a94-8c/.ipykernel/2516/command-5096760691136219-1274913725:111: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  end_ts = dt.datetime.utcnow()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 543 bytes.\nWrote 543 bytes.\nWrote run metadata: /Volumes/rearc_quest/lakehouse/raw_bls/_meta/runs/20260119T010028Z_9d23c934.json\nUpdated latest pointer: /Volumes/rearc_quest/lakehouse/raw_bls/_meta/latest.json\nDone. Target dir: /Volumes/rearc_quest/lakehouse/raw_bls\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Create run_id + run metadata\n",
    "# ----------------------------\n",
    "start_ts = dt.datetime.utcnow()\n",
    "\n",
    "run_utc = start_ts.replace(microsecond=0).isoformat() + \"Z\"\n",
    "run_id_seed = f\"{run_utc}|{DATA_SOURCE}|{TARGET_DIR}\"\n",
    "run_id = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\") + \"_\" + hashlib.sha256(run_id_seed.encode(\"utf-8\")).hexdigest()[:8]\n",
    "\n",
    "RUN_META_PATH = f\"{RUNS_DIR}/{run_id}.json\"\n",
    "\n",
    "run_info = {\n",
    "    \"run_id\": run_id,\n",
    "    \"run_utc\": run_utc,\n",
    "    \"source\": DATA_SOURCE,\n",
    "    \"target_dir\": TARGET_DIR,\n",
    "\n",
    "    \"delete_enabled\": ENABLE_DELETE,\n",
    "    \"protected_files\": sorted(list(PROTECTED_FILES)),\n",
    "\n",
    "    \"remote_file_count\": None,\n",
    "    \"local_file_count_start\": None,\n",
    "\n",
    "    \"uploaded\": 0,\n",
    "    \"updated\": 0,\n",
    "    \"skipped\": 0,\n",
    "    \"deleted\": 0,\n",
    "\n",
    "    \"status\": None,           # \"success\" | \"partial_success\" | \"failed\"\n",
    "    \"errors\": [],\n",
    "\n",
    "    \"started_utc\": run_utc,\n",
    "    \"ended_utc\": None,\n",
    "    \"duration_seconds\": None\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 1) Snapshot local state\n",
    "    local_files = set(list_local_files(TARGET_DIR))\n",
    "    run_info[\"local_file_count_start\"] = len(local_files)\n",
    "    delete_candidates = set(local_files)\n",
    "\n",
    "    # 2) Snapshot remote state\n",
    "    remote_files = list_remote_files()\n",
    "    run_info[\"remote_file_count\"] = len(remote_files)\n",
    "    print(\"Remote files:\", len(remote_files))\n",
    "\n",
    "    # 3) Download & sync (strict full-file hash compare)\n",
    "    for file_name in remote_files:\n",
    "        if not file_name or file_name.strip() == \"\" or file_name == \"[To Parent Directory]\":\n",
    "            continue\n",
    "\n",
    "        url = DATA_SOURCE + file_name\n",
    "        dst_path = f\"{TARGET_DIR}/{file_name}\"\n",
    "\n",
    "        try:\n",
    "            resp = session.get(url, timeout=120)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            remote_content = resp.content\n",
    "            remote_hash = sha256_bytes(remote_content)\n",
    "\n",
    "            if file_name not in local_files:\n",
    "                # New file\n",
    "                dbutils.fs.put(dst_path, remote_content.decode(\"utf-8\", errors=\"replace\"), overwrite=True)\n",
    "                run_info[\"uploaded\"] += 1\n",
    "            else:\n",
    "                # Existing file: strict correctness full-file hash\n",
    "                existing_bytes = read_local_bytes(dst_path)\n",
    "                existing_hash = sha256_bytes(existing_bytes)\n",
    "\n",
    "                if existing_hash != remote_hash:\n",
    "                    dbutils.fs.put(dst_path, remote_content.decode(\"utf-8\", errors=\"replace\"), overwrite=True)\n",
    "                    run_info[\"updated\"] += 1\n",
    "                else:\n",
    "                    run_info[\"skipped\"] += 1\n",
    "\n",
    "            # If it exists upstream, it's not a delete candidate\n",
    "            delete_candidates.discard(file_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            run_info[\"errors\"].append({\"file\": file_name, \"url\": url, \"error\": str(e)})\n",
    "\n",
    "    print(f\"Uploaded={run_info['uploaded']}, Updated={run_info['updated']}, Skipped={run_info['skipped']}\")\n",
    "\n",
    "    # 4) Delete removed files (mirror behavior)\n",
    "    if ENABLE_DELETE:\n",
    "        deleted = 0\n",
    "        for file_name in list(delete_candidates):\n",
    "            if file_name in PROTECTED_FILES:\n",
    "                continue\n",
    "            try:\n",
    "                dbutils.fs.rm(f\"{TARGET_DIR}/{file_name}\", recurse=False)\n",
    "                deleted += 1\n",
    "            except Exception as e:\n",
    "                run_info[\"errors\"].append({\"file\": file_name, \"url\": None, \"error\": f\"delete_failed: {str(e)}\"})\n",
    "        run_info[\"deleted\"] = deleted\n",
    "        print(\"Deleted files:\", deleted)\n",
    "\n",
    "    # 5) Status\n",
    "    run_info[\"status\"] = \"partial_success\" if run_info[\"errors\"] else \"success\"\n",
    "    if run_info[\"errors\"]:\n",
    "        print(f\"Errors: {len(run_info['errors'])} (see per-run JSON in _meta/runs/)\")\n",
    "\n",
    "except Exception as e:\n",
    "    run_info[\"status\"] = \"failed\"\n",
    "    run_info[\"errors\"].append({\"file\": None, \"url\": DATA_SOURCE, \"error\": f\"fatal: {str(e)}\"})\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    end_ts = dt.datetime.utcnow()\n",
    "    run_info[\"ended_utc\"] = end_ts.replace(microsecond=0).isoformat() + \"Z\"\n",
    "    run_info[\"duration_seconds\"] = int((end_ts - start_ts).total_seconds())\n",
    "\n",
    "    # Always write per-run metadata + latest pointer\n",
    "    dbutils.fs.put(RUN_META_PATH, json.dumps(run_info, indent=2), overwrite=False)\n",
    "    dbutils.fs.put(LATEST_PATH, json.dumps(run_info, indent=2), overwrite=True)\n",
    "\n",
    "    print(\"Wrote run metadata:\", RUN_META_PATH)\n",
    "    print(\"Updated latest pointer:\", LATEST_PATH)\n",
    "    print(\"Done. Target dir:\", TARGET_DIR)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5841774949002084,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "10_ingest_bls",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}